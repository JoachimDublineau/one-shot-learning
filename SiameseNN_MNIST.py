# -*- coding: utf-8 -*-
"""SiameseNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ahBFLABq2v3CcoYNzsQRHvUxFEf_x7E7
"""

from keras.datasets import mnist, cifar10
from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Input, Lambda, Dropout
from keras.layers.core import Reshape
from keras.utils import to_categorical
from keras.initializers import RandomNormal
from keras.regularizers import l2
from keras.optimizers import Adam, RMSprop, SGD
from keras import backend as K

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score


#############################################
#  prepare data
#############################################
class Data_MNIST:
  def __init__(self):
    self.prepare_data()
    self.group_data()

  def prepare_data(self):
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    self.img_rows = x_train.shape[1]
    self.img_cols = x_train.shape[2]
    self.input_shape = (self.img_rows*self.img_cols,)

    x_train = x_train.reshape(-1, self.img_rows*self.img_cols)
    x_test = x_test.reshape(-1, self.img_rows*self.img_cols)
    x_train = x_train.astype("float32")
    x_test = x_test.astype("float32")
    x_train /= 255.
    x_test /= 255.
    self.x_train = x_train
    self.x_test = x_test
    self.y_train = y_train
    self.y_test = y_test

  def group_data(self):
    self.grouped_data = {}
    for i in range(10):
      # print(i)
      # print(self.y_train.shape, self.x_train.shape)
      self.grouped_data[i] = self.x_train[self.y_train==i]

  def get_batch(self, num_list):
    target = []
    num_example = min([len(self.grouped_data[i]) for i in num_list])
    batch = [np.zeros((num_example * len(num_list), self.img_rows * self.img_cols)) for i in range(2)]

    for i in range(num_example * len(num_list)):
      i1 = np.random.choice(num_list)
      if i%2 == 0:
        i2 = i1
        target.append(1)
      else:
        i2 = num_list[(np.random.choice(len(num_list)-1)+i1)%len(num_list)]
        target.append(0)
        
      i1_index = np.random.choice(self.grouped_data[i1].shape[0])
      i2_index = np.random.choice(self.grouped_data[i2].shape[0])
      batch[0][i,:] = self.grouped_data[i1][i1_index]
      batch[1][i,:] = self.grouped_data[i2][i2_index]
    return batch, target

  def get_val_batch(self, batch_size, num_list):
    batch = [np.zeros((batch_size, self.img_rows * self.img_cols)) for i in range(2)]
    target = []

    for i in range(batch_size):
      i1 = np.random.choice(num_list)
      if i%2 == 0:
        i2 = i1
        target.append(1)
      else:
        i2 = num_list[(np.random.choice(len(num_list)-1)+i1)%len(num_list)]
        target.append(0)
        
      i1_index = np.random.choice(self.grouped_data[i1].shape[0])
      i2_index = np.random.choice(self.grouped_data[i2].shape[0])
      batch[0][i,:] = self.grouped_data[i1][i1_index]
      batch[1][i,:] = self.grouped_data[i2][i2_index]
    return batch, target

  def get_test_batch(self, test_size, k=10):#k-way one shot learning
    batch = [np.zeros((test_size*k, self.img_rows * self.img_cols)) for i in range(2)]
    target = [] # index of correct category
    num_list = list(range(7,10))
    categ_list = list(range(10))
    # categ_list = np.random.choice(num_list, k, replace=False)
    for i in range(test_size):
      i1 = np.random.choice(num_list)
      i1_index = np.random.choice(self.grouped_data[i1].shape[0])
      batch[0][i*k:i*k+k,:] = np.repeat(self.grouped_data[i1][i1_index][np.newaxis,:], k, axis=0)
      target.append(i1)
      for k_i in range(k):
        i2_index = np.random.choice(self.grouped_data[categ_list[k_i]].shape[0])
        batch[1][i*k+k_i,:] = self.grouped_data[categ_list[k_i]][i2_index]
        while categ_list[k_i]==i1 and i2_index == i1_index:
          i2_index = np.random.choice(self.grouped_data[categ_list[k_i]].shape[0])
          batch[1][i*k+k_i,:] = self.grouped_data[categ_list[k_i]][i2_index]
      
    return batch, target

#############################################
#  define euclidean distance
#############################################
def euclidean_distance(vects):
    x, y = vects
    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)
    return K.sqrt(K.maximum(sum_square, K.epsilon()))

def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)

#############################################
#  define siamese model
#############################################
def get_siamese_model(input_shape, distance = 'l1'):
    
  left_input = Input(input_shape)
  right_input = Input(input_shape)

  model = Sequential()  
  model.add(Dense(1024, activation='relu', input_shape=input_shape))
  model.add(Dropout(0.2))
  model.add(Dense(256, activation='relu'))

  encoded_l = model(left_input)
  encoded_r = model(right_input)
  if distance == 'l1':
    
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    prediction = Dense(1,activation='sigmoid')(L1_distance)

    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)

  else:
    distance = Lambda(euclidean_distance,
                  output_shape=eucl_dist_output_shape)([encoded_l, encoded_r])

    siamese_net = Model([left_input,right_input], distance)
  return siamese_net

def test_model(data, model):
  test_size = 128
  batch, target = data.get_test_batch(test_size)
  pre = model.predict(batch)
  return pre, target

def contrastive_loss(y_true, y_pred):
  '''Contrastive loss from Hadsell-et-al.'06
  http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
  '''
  margin = 1
  square_pred = K.square(y_pred)
  margin_square = K.square(K.maximum(margin - y_pred, 0))
  return K.mean(y_true * square_pred + (1 - y_true) * margin_square)

def accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))

#############################################
#  Plots
#############################################
def plot_history(history, with_val = False):
  history_dict = history.history
  loss_values = history_dict['loss']
  if with_val:
    val_loss_values = history_dict['val_loss']
  epochs = range(1, len(loss_values) + 1)
  plt.figure(figsize=(15,5))
  plt.subplot(1,2,1)
  plt.plot(epochs, loss_values, label='Training loss')
  if with_val:
    plt.plot(epochs, val_loss_values, label='Validation loss')
  plt.title('Training loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()

  acc = history_dict['accuracy']
  if with_val:
    val_acc = history_dict['val_accuracy']
  plt.subplot(1,2,2)
  plt.plot(epochs, acc, label='Training acc')
  if with_val:
    plt.plot(epochs, val_acc, label='Validation acc')
  plt.title('Training acc')
  plt.xlabel('Epochs')
  plt.ylabel('acc')
  plt.legend()
  
  plt.show()


if __name__ == '__main__':
  data = Data_MNIST()
  # distance = 'l1'
  distance = 'euclidean'
  model = get_siamese_model(data.input_shape, distance)

  # optimizer = Adam(lr = 1e-5)
  # optimizer = SGD(lr=0.1)
  optimizer = RMSprop()

  if distance == 'l1':
    model.compile(loss="binary_crossentropy",optimizer=optimizer, metrics=['accuracy'])
  else:
    model.compile(loss=contrastive_loss,optimizer=optimizer, metrics=[accuracy])

  epochs = 40
  batch_size = 1024
  train_num_list = list(range(5))
  batch, target = data.get_batch(train_num_list)

  val_num_list = list(range(10))
  history = model.fit(batch, target, batch_size = batch_size, verbose=1, 
                                epochs=epochs, validation_data=data.get_val_batch(batch_size, val_num_list))

  plot_history(history, with_val=True)

  y_pre,target = test_model(data, model)
  pre = np.argmin(y_pre.reshape(-1,10),axis=1)
  print(accuracy_score(target, pre))
