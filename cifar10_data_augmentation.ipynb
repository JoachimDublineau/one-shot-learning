{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_mestests_dataaug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqlJnhQh9Ft8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7539a568-b5c6-403b-dd94-8f7ab9126df6"
      },
      "source": [
        "!curl -O https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  162M  100  162M    0     0  13.6M      0  0:00:11  0:00:11 --:--:-- 16.7M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF6IwApVCU2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "207483bd-d193-4499-b291-89e4186dd12e"
      },
      "source": [
        "!tar xvzf cifar-10-python.tar.gz"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1c0cE_fCtf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_cifar10_data(batch_number):\n",
        "  with open('cifar-10-batches-py/data_batch_'+ str(batch_number), 'rb') as file:\n",
        "      batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "  features = batch['data']\n",
        "  labels = batch['labels']\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnam-T03F9TW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_1, labels_1 = load_cifar10_data(1)\n",
        "batch_2, labels_2 = load_cifar10_data(2)\n",
        "batch_3, labels_3 = load_cifar10_data(3)\n",
        "batch_4, labels_4 = load_cifar10_data(4)\n",
        "batch_5, labels_5 = load_cifar10_data(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7j-zoeWJ8tT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0a48832d-5f8a-43d2-d97f-f25578dcd73b"
      },
      "source": [
        "batch_3"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 26,  17,  13, ...,  27,  26,  27],\n",
              "       [ 94, 101,  95, ..., 182, 184, 155],\n",
              "       [183, 158, 166, ..., 250, 250, 250],\n",
              "       ...,\n",
              "       [175, 200, 207, ..., 124,  49,  32],\n",
              "       [ 28,  59,  67, ...,  36,  44,  41],\n",
              "       [ 62,  40,  61, ..., 127, 124, 116]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz9baQEdGBgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "all_images = np.append(batch_1, batch_2, axis=0)\n",
        "all_images = all_images.reshape((len(all_images), 3, 32, 32)).transpose(0,2,3,1)\n",
        "all_labels = np.append(labels_1, labels_2, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5MTMXUnK8tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.append(batch_1[0:8000], batch_2[0:8000], axis=0)\n",
        "X_train = np.append(X_train, batch_3[0:8000], axis=0)\n",
        "X_train = np.append(X_train, batch_4[0:8000], axis=0)\n",
        "X_train = np.append(X_train, batch_5[0:8000], axis=0)\n",
        "\n",
        "Y_train = np.append(labels_1[0:8000], labels_2[0:8000], axis=0)\n",
        "Y_train = np.append(Y_train, labels_3[0:8000], axis=0)\n",
        "Y_train = np.append(Y_train, labels_4[0:8000], axis=0)\n",
        "Y_train = np.append(Y_train, labels_5[0:8000], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbef2Ah9K8wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_validation = np.append(batch_1[8001:9000], batch_2[8001:9000], axis=0)\n",
        "X_validation = np.append(X_validation, batch_3[8001:9000], axis=0)\n",
        "X_validation = np.append(X_validation, batch_4[8001:9000], axis=0)\n",
        "X_validation = np.append(X_validation, batch_5[8001:9000], axis=0)\n",
        "\n",
        "Y_validation = np.append(labels_1[8001:9000], labels_2[8001:9000], axis=0)\n",
        "Y_validation = np.append(Y_validation, labels_3[8001:9000], axis=0)\n",
        "Y_validation = np.append(Y_validation, labels_4[8001:9000], axis=0)\n",
        "Y_validation = np.append(Y_validation, labels_5[8001:9000], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq9cHd71LI8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.append(batch_1[9001:10000], batch_2[9001:10000], axis=0)\n",
        "X_test = np.append(X_test, batch_3[9001:10000], axis=0)\n",
        "X_test = np.append(X_test, batch_4[9001:10000], axis=0)\n",
        "X_test = np.append(X_test, batch_5[9001:10000], axis=0)\n",
        "\n",
        "Y_test = np.append(labels_1[9001:10000], labels_2[9001:10000], axis=0)\n",
        "Y_test = np.append(Y_test, labels_3[9001:10000], axis=0)\n",
        "Y_test = np.append(Y_test, labels_4[9001:10000], axis=0)\n",
        "Y_test = np.append(Y_test, labels_5[9001:10000], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDzyDSxU83Vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9807d57e-4198-4d33-d9b7-6cd9074d3405"
      },
      "source": [
        "print(\"Length of X_train:\", len(X_train), \"Length of Y_train:\", len(Y_train))\n",
        "print(\"Length of X_validation:\",len(X_validation), \"Length of Y_validation:\", len(Y_validation))\n",
        "print(\"Length of X_test:\",len(X_test), \"Length of Y_test:\", len(Y_test))"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of X_train: 40000 Length of Y_train: 40000\n",
            "Length of X_validation: 4995 Length of Y_validation: 4995\n",
            "Length of X_test: 4995 Length of Y_test: 4995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReszAqneMnwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " %tensorflow_version 1.x\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Activation, \\\n",
        "Flatten, Conv3D, MaxPooling3D\n",
        "from keras import regularizers\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFc9z6QoLal5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_one_hot = np_utils.to_categorical(Y_train, 10)\n",
        "Y_validation_one_hot = np_utils.to_categorical(Y_validation, 10)\n",
        "Y_test_one_hot = np_utils.to_categorical(Y_test, 10)\n",
        "\n",
        "X_train = X_train.reshape((len(X_train), 3, 32, 32)).transpose(0,2,3,1)\n",
        "X_validation = X_validation.reshape((len(X_validation), 3, 32, 32)).transpose(0,2,3,1)\n",
        "X_test = X_test.reshape((len(X_test), 3, 32, 32)).transpose(0,2,3,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yudnlRHO8MD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#defining parameters\n",
        "\n",
        "epochs=110\n",
        "nb_classes=10\n",
        "conv_1_depth=100\n",
        "conv_2_depth=100\n",
        "conv_3_depth=200\n",
        "conv_4_depth=200\n",
        "conv_5_depth=400\n",
        "dense_1_depth=600\n",
        "dense_2_depth=nb_classes\n",
        "batch_size=32\n",
        "learning_rate=0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZVxNjI98dW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e999ce3-a1fe-4f25-e6e4-1ab359300344"
      },
      "source": [
        "#defining simple CNN model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(conv_1_depth, (3, 3), padding='same',\n",
        "                 input_shape=X_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(conv_2_depth, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(conv_3_depth, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(conv_4_depth, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(conv_5_depth, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(dense_1_depth,kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(dense_2_depth))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#show model summary\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "#compile model \n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=2, width_shift_range=0.15, height_shift_range=0.15, shear_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
        "\n",
        "train_set = train_datagen.flow(X_train, Y_train_one_hot, batch_size=32)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_set = validation_datagen.flow(X_validation, Y_validation_one_hot, batch_size=32)\n",
        "\n",
        "history = model.fit_generator(train_set,\n",
        "                    steps_per_epoch=600,epochs=epochs,\n",
        "                    validation_data=(validation_set), validation_steps=300, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_66 (Conv2D)           (None, 32, 32, 100)       2800      \n",
            "_________________________________________________________________\n",
            "activation_92 (Activation)   (None, 32, 32, 100)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 32, 32, 100)       90100     \n",
            "_________________________________________________________________\n",
            "activation_93 (Activation)   (None, 32, 32, 100)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 16, 16, 100)       0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 16, 16, 100)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 16, 16, 200)       180200    \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 16, 16, 200)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 16, 16, 200)       360200    \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 16, 16, 200)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 16, 16, 400)       720400    \n",
            "_________________________________________________________________\n",
            "activation_96 (Activation)   (None, 16, 16, 400)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 8, 8, 400)         0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 8, 8, 400)         0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 600)               15360600  \n",
            "_________________________________________________________________\n",
            "activation_97 (Activation)   (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                6010      \n",
            "_________________________________________________________________\n",
            "activation_98 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,720,310\n",
            "Trainable params: 16,720,310\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/110\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 2.0927 - acc: 0.2797 - val_loss: 1.7456 - val_acc: 0.4065\n",
            "Epoch 2/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.7625 - acc: 0.4026 - val_loss: 1.5492 - val_acc: 0.4893\n",
            "Epoch 3/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.6548 - acc: 0.4419 - val_loss: 1.5030 - val_acc: 0.4976\n",
            "Epoch 4/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.5379 - acc: 0.4864 - val_loss: 1.3394 - val_acc: 0.5639\n",
            "Epoch 5/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.4564 - acc: 0.5238 - val_loss: 1.4582 - val_acc: 0.5466\n",
            "Epoch 6/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.4009 - acc: 0.5420 - val_loss: 1.1858 - val_acc: 0.6279\n",
            "Epoch 7/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.3406 - acc: 0.5624 - val_loss: 1.1439 - val_acc: 0.6388\n",
            "Epoch 8/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.3106 - acc: 0.5807 - val_loss: 1.1624 - val_acc: 0.6312\n",
            "Epoch 9/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.2669 - acc: 0.5974 - val_loss: 1.2404 - val_acc: 0.6222\n",
            "Epoch 10/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.2388 - acc: 0.6119 - val_loss: 1.1461 - val_acc: 0.6442\n",
            "Epoch 11/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.1996 - acc: 0.6258 - val_loss: 1.1016 - val_acc: 0.6626\n",
            "Epoch 12/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.1812 - acc: 0.6321 - val_loss: 1.0631 - val_acc: 0.6766\n",
            "Epoch 13/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.1505 - acc: 0.6468 - val_loss: 1.0781 - val_acc: 0.6694\n",
            "Epoch 14/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.1303 - acc: 0.6543 - val_loss: 1.0441 - val_acc: 0.6914\n",
            "Epoch 15/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.1075 - acc: 0.6670 - val_loss: 1.1010 - val_acc: 0.6785\n",
            "Epoch 16/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0923 - acc: 0.6705 - val_loss: 1.0335 - val_acc: 0.6953\n",
            "Epoch 17/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0651 - acc: 0.6842 - val_loss: 1.0099 - val_acc: 0.7074\n",
            "Epoch 18/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0511 - acc: 0.6874 - val_loss: 0.9766 - val_acc: 0.7100\n",
            "Epoch 19/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0406 - acc: 0.6935 - val_loss: 0.9205 - val_acc: 0.7377\n",
            "Epoch 20/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0200 - acc: 0.7014 - val_loss: 0.9973 - val_acc: 0.7108\n",
            "Epoch 21/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 1.0131 - acc: 0.7044 - val_loss: 0.9547 - val_acc: 0.7285\n",
            "Epoch 22/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9980 - acc: 0.7102 - val_loss: 0.8811 - val_acc: 0.7551\n",
            "Epoch 23/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9891 - acc: 0.7176 - val_loss: 0.9828 - val_acc: 0.7199\n",
            "Epoch 24/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9761 - acc: 0.7212 - val_loss: 0.8526 - val_acc: 0.7625\n",
            "Epoch 25/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9612 - acc: 0.7264 - val_loss: 0.9048 - val_acc: 0.7484\n",
            "Epoch 26/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9503 - acc: 0.7291 - val_loss: 0.8209 - val_acc: 0.7802\n",
            "Epoch 27/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9503 - acc: 0.7324 - val_loss: 0.8187 - val_acc: 0.7783\n",
            "Epoch 28/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9294 - acc: 0.7401 - val_loss: 0.8068 - val_acc: 0.7809\n",
            "Epoch 29/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9353 - acc: 0.7428 - val_loss: 0.9096 - val_acc: 0.7553\n",
            "Epoch 30/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9151 - acc: 0.7507 - val_loss: 0.8514 - val_acc: 0.7790\n",
            "Epoch 31/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.9163 - acc: 0.7498 - val_loss: 0.8620 - val_acc: 0.7644\n",
            "Epoch 32/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8970 - acc: 0.7584 - val_loss: 0.8458 - val_acc: 0.7737\n",
            "Epoch 33/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8999 - acc: 0.7563 - val_loss: 0.8207 - val_acc: 0.7833\n",
            "Epoch 34/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8874 - acc: 0.7601 - val_loss: 0.8194 - val_acc: 0.7861\n",
            "Epoch 35/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8757 - acc: 0.7648 - val_loss: 0.8739 - val_acc: 0.7639\n",
            "Epoch 36/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8896 - acc: 0.7592 - val_loss: 0.7832 - val_acc: 0.7957\n",
            "Epoch 37/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8695 - acc: 0.7684 - val_loss: 0.8564 - val_acc: 0.7759\n",
            "Epoch 38/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8571 - acc: 0.7749 - val_loss: 0.7937 - val_acc: 0.7951\n",
            "Epoch 39/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8703 - acc: 0.7711 - val_loss: 0.8058 - val_acc: 0.7925\n",
            "Epoch 40/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8562 - acc: 0.7759 - val_loss: 0.8028 - val_acc: 0.7961\n",
            "Epoch 41/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8444 - acc: 0.7805 - val_loss: 0.7765 - val_acc: 0.8061\n",
            "Epoch 42/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8468 - acc: 0.7833 - val_loss: 0.7541 - val_acc: 0.8084\n",
            "Epoch 43/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8446 - acc: 0.7814 - val_loss: 0.8008 - val_acc: 0.7958\n",
            "Epoch 44/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8275 - acc: 0.7886 - val_loss: 0.7367 - val_acc: 0.8211\n",
            "Epoch 45/110\n",
            "600/600 [==============================] - 15s 24ms/step - loss: 0.8315 - acc: 0.7818 - val_loss: 0.7336 - val_acc: 0.8212\n",
            "Epoch 46/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8232 - acc: 0.7872 - val_loss: 0.7418 - val_acc: 0.8190\n",
            "Epoch 47/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8144 - acc: 0.7906 - val_loss: 0.7938 - val_acc: 0.8011\n",
            "Epoch 48/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8186 - acc: 0.7898 - val_loss: 0.7729 - val_acc: 0.8087\n",
            "Epoch 49/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8022 - acc: 0.7960 - val_loss: 0.7033 - val_acc: 0.8340\n",
            "Epoch 50/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8215 - acc: 0.7927 - val_loss: 0.7484 - val_acc: 0.8122\n",
            "Epoch 51/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7898 - acc: 0.8015 - val_loss: 0.7293 - val_acc: 0.8227\n",
            "Epoch 52/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8052 - acc: 0.7990 - val_loss: 0.7251 - val_acc: 0.8272\n",
            "Epoch 53/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7898 - acc: 0.8028 - val_loss: 0.7348 - val_acc: 0.8231\n",
            "Epoch 54/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.8030 - acc: 0.7991 - val_loss: 0.7911 - val_acc: 0.8048\n",
            "Epoch 55/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7864 - acc: 0.8072 - val_loss: 0.7042 - val_acc: 0.8361\n",
            "Epoch 56/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7841 - acc: 0.8072 - val_loss: 0.7906 - val_acc: 0.8061\n",
            "Epoch 57/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7685 - acc: 0.8135 - val_loss: 0.8189 - val_acc: 0.8091\n",
            "Epoch 58/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7826 - acc: 0.8090 - val_loss: 0.7143 - val_acc: 0.8322\n",
            "Epoch 59/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7751 - acc: 0.8098 - val_loss: 0.7267 - val_acc: 0.8280\n",
            "Epoch 60/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7732 - acc: 0.8129 - val_loss: 0.7550 - val_acc: 0.8167\n",
            "Epoch 61/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7655 - acc: 0.8180 - val_loss: 0.7306 - val_acc: 0.8295\n",
            "Epoch 62/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7656 - acc: 0.8145 - val_loss: 0.7773 - val_acc: 0.8094\n",
            "Epoch 63/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7610 - acc: 0.8165 - val_loss: 0.7859 - val_acc: 0.8128\n",
            "Epoch 64/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7417 - acc: 0.8223 - val_loss: 0.7092 - val_acc: 0.8359\n",
            "Epoch 65/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7590 - acc: 0.8201 - val_loss: 0.7470 - val_acc: 0.8207\n",
            "Epoch 66/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7444 - acc: 0.8231 - val_loss: 0.6893 - val_acc: 0.8444\n",
            "Epoch 67/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7571 - acc: 0.8168 - val_loss: 0.7276 - val_acc: 0.8240\n",
            "Epoch 68/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7403 - acc: 0.8301 - val_loss: 0.7077 - val_acc: 0.8415\n",
            "Epoch 69/110\n",
            "600/600 [==============================] - 15s 24ms/step - loss: 0.7374 - acc: 0.8247 - val_loss: 0.7415 - val_acc: 0.8298\n",
            "Epoch 70/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7425 - acc: 0.8247 - val_loss: 0.7920 - val_acc: 0.8201\n",
            "Epoch 71/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7381 - acc: 0.8243 - val_loss: 0.7513 - val_acc: 0.8277\n",
            "Epoch 72/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7271 - acc: 0.8318 - val_loss: 0.6705 - val_acc: 0.8543\n",
            "Epoch 73/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7402 - acc: 0.8285 - val_loss: 0.6833 - val_acc: 0.8515\n",
            "Epoch 74/110\n",
            "600/600 [==============================] - 15s 24ms/step - loss: 0.7332 - acc: 0.8295 - val_loss: 0.7319 - val_acc: 0.8317\n",
            "Epoch 75/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7152 - acc: 0.8328 - val_loss: 0.7301 - val_acc: 0.8387\n",
            "Epoch 76/110\n",
            "600/600 [==============================] - 15s 24ms/step - loss: 0.7209 - acc: 0.8346 - val_loss: 0.6914 - val_acc: 0.8449\n",
            "Epoch 77/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7220 - acc: 0.8351 - val_loss: 0.7012 - val_acc: 0.8397\n",
            "Epoch 78/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7143 - acc: 0.8362 - val_loss: 0.7704 - val_acc: 0.8171\n",
            "Epoch 79/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7228 - acc: 0.8335 - val_loss: 0.7278 - val_acc: 0.8389\n",
            "Epoch 80/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7035 - acc: 0.8439 - val_loss: 0.6548 - val_acc: 0.8599\n",
            "Epoch 81/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7056 - acc: 0.8418 - val_loss: 0.6890 - val_acc: 0.8434\n",
            "Epoch 82/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7121 - acc: 0.8355 - val_loss: 0.6711 - val_acc: 0.8530\n",
            "Epoch 83/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6999 - acc: 0.8428 - val_loss: 0.8060 - val_acc: 0.8176\n",
            "Epoch 84/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6972 - acc: 0.8408 - val_loss: 0.6510 - val_acc: 0.8618\n",
            "Epoch 85/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7024 - acc: 0.8406 - val_loss: 0.6952 - val_acc: 0.8473\n",
            "Epoch 86/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6988 - acc: 0.8409 - val_loss: 0.6922 - val_acc: 0.8464\n",
            "Epoch 87/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.7036 - acc: 0.8450 - val_loss: 0.6522 - val_acc: 0.8621\n",
            "Epoch 88/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6852 - acc: 0.8466 - val_loss: 0.7275 - val_acc: 0.8389\n",
            "Epoch 89/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6832 - acc: 0.8479 - val_loss: 0.6781 - val_acc: 0.8545\n",
            "Epoch 90/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6857 - acc: 0.8510 - val_loss: 0.6624 - val_acc: 0.8589\n",
            "Epoch 91/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6907 - acc: 0.8473 - val_loss: 0.6798 - val_acc: 0.8549\n",
            "Epoch 92/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6855 - acc: 0.8483 - val_loss: 0.7576 - val_acc: 0.8324\n",
            "Epoch 93/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6769 - acc: 0.8502 - val_loss: 0.6536 - val_acc: 0.8608\n",
            "Epoch 94/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6805 - acc: 0.8521 - val_loss: 0.6591 - val_acc: 0.8627\n",
            "Epoch 95/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6656 - acc: 0.8556 - val_loss: 0.6831 - val_acc: 0.8563\n",
            "Epoch 96/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6830 - acc: 0.8497 - val_loss: 0.7122 - val_acc: 0.8476\n",
            "Epoch 97/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6687 - acc: 0.8553 - val_loss: 0.6806 - val_acc: 0.8532\n",
            "Epoch 98/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6747 - acc: 0.8510 - val_loss: 0.6734 - val_acc: 0.8579\n",
            "Epoch 99/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6744 - acc: 0.8508 - val_loss: 0.6847 - val_acc: 0.8579\n",
            "Epoch 100/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6675 - acc: 0.8580 - val_loss: 0.8076 - val_acc: 0.8245\n",
            "Epoch 101/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6549 - acc: 0.8600 - val_loss: 0.7109 - val_acc: 0.8463\n",
            "Epoch 102/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6627 - acc: 0.8597 - val_loss: 0.6920 - val_acc: 0.8540\n",
            "Epoch 103/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6603 - acc: 0.8581 - val_loss: 0.6843 - val_acc: 0.8539\n",
            "Epoch 104/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6651 - acc: 0.8569 - val_loss: 0.6534 - val_acc: 0.8660\n",
            "Epoch 105/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6625 - acc: 0.8569 - val_loss: 0.6478 - val_acc: 0.8687\n",
            "Epoch 106/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6586 - acc: 0.8613 - val_loss: 0.6368 - val_acc: 0.8733\n",
            "Epoch 107/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6435 - acc: 0.8635 - val_loss: 0.6815 - val_acc: 0.8582\n",
            "Epoch 108/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6519 - acc: 0.8631 - val_loss: 0.6847 - val_acc: 0.8572\n",
            "Epoch 109/110\n",
            "600/600 [==============================] - 15s 25ms/step - loss: 0.6519 - acc: 0.8645 - val_loss: 0.6512 - val_acc: 0.8652\n",
            "Epoch 110/110\n",
            "600/600 [==============================] - 15s 24ms/step - loss: 0.6565 - acc: 0.8628 - val_loss: 0.6575 - val_acc: 0.8654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovA3fGZe8dtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6e13a949-064f-4543-86b3-b2a683c184c7"
      },
      "source": [
        "#plot training and validation sets' losses\n",
        "\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAdkq53g299vPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQkvEgs8gOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a9491c12-e0b2-4465-84d7-209fb6ea0385"
      },
      "source": [
        "#evaluate on the test set \n",
        "X_test = (X_test)*1./255 \n",
        "print(model.evaluate(X_test, Y_test_one_hot, batch_size=32))\n",
        "\n"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4995/4995 [==============================] - 1s 168us/step\n",
            "[0.6736694541719702, 0.8640640640640641]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}